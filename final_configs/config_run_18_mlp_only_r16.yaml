# Configuration for 25k subset, 3 epochs, multi-GPU batch job
# Paths updated for final_configs and final_outputs directories

wandb:
  project: "Last minute Test EM F1" # Adjusted project name

model:
  name: "mistralai/Mistral-7B-v0.1"
  # MODIFIED output directory path for run_18_mlp_only_r16
  output_dir: "/home/hice1/rstonick3/scratch/titan-project/final_outputs/run_18_mlp_only_r16"
  wandb_run_name: "run_18_mlp_only_r16" # Updated run name

quantization:
  load_in_4bit: True
  use_double_quant: True
  quant_type: "nf4"
  compute_dtype: "bfloat16"

lora:
  basic:
    r: 16 # Updated for run 18
    alpha: 32 # Updated for run 18
    dropout: 0.05
  target_modules:
    # Removed q, k, v, o
    - "gate_proj" # Updated for run 18 (MLP only)
    - "up_proj"   # Updated for run 18 (MLP only)
    - "down_proj" # Updated for run 18 (MLP only)

  advanced:
    bias: "none"
    task_type: "CAUSAL_LM"
    fan_in_fan_out: False
    modules_to_save: null
    init_lora_weights: True
  rank_pattern: {} # Ensure empty for run 18
  alpha_pattern: {} # Ensure empty for run 18

dataset:
  # REMOVED dataset_name and dataset_config as we use local files now
  # dataset_name: "trivia_qa"
  # dataset_config: "rc.nocontext"
  # ADDED local path and file specifications
  local_path: /home/hice1/rstonick3/scratch/triviaqa_dataset # Path to the root dir containing qa/
  train_files:
    - qa/wikipedia-train.json
    # - qa/web-train.json # Uncomment to include web data
  validation_files:
    - qa/wikipedia-dev.json
    # - qa/web-dev.json # Uncomment to include web data
  subset_size: 2000
  # ADDED prompt format (adjust if your training script expects differently)
  prompt_format: "Question: {question}\\nAnswer: {answer_value}"

training:
  batch_size: 128 # Per-device batch size (Effective BS = 1 * 96 = 96)
  gradient_accumulation_steps: 1
  num_epochs: 3
  learning_rate: 5.6e-5 # Adjusted for 1 GPU (was 1.12e-4)
  warmup_ratio: 0.1
  max_length: 512

trainer:
  # Steps adjusted for ~105 steps/epoch (Effective BS 192) * 3 epochs = ~315 total steps
  logging_steps: 30 # Adjusted from 20
  save_strategy: "epoch" # Changed from "steps"
  # save_steps: 100 # Commented out as strategy is now "epoch"
  evaluation_strategy: "epoch" # CORRECTED VALUE for eval_steps
  eval_steps: 200 # Added for evaluation steps
  per_device_eval_batch_size: 4 # Added for evaluation batch size
  #eval_accumulation_steps: 4 # Added for evaluation accumulation steps
  dataloader_num_workers: 8
  optim: "paged_adamw_8bit"
  lr_scheduler_type: "cosine"
  fp16: False
  bf16: True
  weight_decay: 0.0

