# filepath: /home/hice1/rstonick3/scratch/titan-project/final_configs/config_run_28_mlp_med_r_all_high_alpha.yaml
# Configuration for 25k subset, 3 epochs, multi-GPU batch job
# Paths updated for final_configs and final_outputs directories

wandb:
  project: "20K-Shima-Table" # Adjusted project name

model:
  name: "mistralai/Mistral-7B-v0.1"
  # MODIFIED output directory path for run_28_mlp_med_r_all_high_alpha
  output_dir: "/home/hice1/rstonick3/scratch/titan-project/final_outputs/run_28_mlp_med_r_all_high_alpha"
  wandb_run_name: "run_28_mlp_med_r_all_high_alpha" # Updated run name

quantization:
  load_in_4bit: True
  use_double_quant: True
  quant_type: "nf4"
  compute_dtype: "bfloat16"

lora:
  basic:
    r: 16 # Updated for run 28
    alpha: 64 # Updated for run 28 (alpha = 4*r)
    dropout: 0.05
  target_modules:
    # Targeting all MLP layers
    - "gate_proj"
    - "up_proj"
    - "down_proj"

  advanced:
    bias: "none"
    task_type: "CAUSAL_LM"
    fan_in_fan_out: False
    modules_to_save: null
    init_lora_weights: True
  rank_pattern: {}
  alpha_pattern: {}

dataset:
  dataset_name: "trivia_qa"
  dataset_config: "rc.nocontext"
  subset_size: 20000 # Kept default

training:
  batch_size: 96 # Per-device batch size (Effective BS = 1 * 96 = 96)
  gradient_accumulation_steps: 1
  num_epochs: 3
  learning_rate: 8e-5
  warmup_ratio: 0.15 # Changed from 0.1
  max_length: 512 # Reduced from 1024

trainer:
  # Steps adjusted for ~105 steps/epoch (Effective BS 96) * 3 epochs = ~315 total steps
  logging_steps: 30
  save_strategy: "steps" # Changed from epoch to support early stopping save
  save_steps: 60 # Save checkpoints more often for early stopping
  evaluation_strategy: "steps" # Changed from "no"
  eval_steps: 60 # Evaluate every 60 steps
  load_best_model_at_end: True # Enable early stopping
  metric_for_best_model: "loss" # Use eval loss for early stopping
  greater_is_better: False # Lower loss is better
  per_device_eval_batch_size: 2 # Reduced from 8
  # eval_accumulation_steps: 4 # Ignored
  dataloader_num_workers: 8
  optim: "paged_adamw_8bit"
  lr_scheduler_type: "cosine"
  fp16: False
  bf16: True
  weight_decay: 0.0
  save_total_limit: 2 # Keep only the best and the latest checkpoint
