Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.23s/it]
trainable params: 19,922,944 || all params: 7,261,655,040 || trainable%: 0.2744
Loading 1000 examples from TriviaQA dataset...
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 61.82it/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 59.98it/s]
Loaded 900 training examples and 100 validation examples
Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 900/900 [00:00<00:00, 4842.87 examples/s]
Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 3628.51 examples/s]
After filtering: 863 training examples and 95 validation examples
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 863/863 [00:00<00:00, 10635.37 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 5430.59 examples/s]
Tokenizing datasets...
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 863/863 [00:00<00:00, 2550.74 examples/s]
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:00<00:00, 2079.56 examples/s]
Final training examples: 863
Final validation examples: 95
No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                          | 0/162 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  6%|â–ˆâ–ˆâ–ˆâ–Ž                                                  | 10/162 [32:25<8:04:07, 191.10s/it]Traceback (most recent call last):                                                                                               
{'loss': 2.0504, 'grad_norm': 7.543986797332764, 'learning_rate': 0.0001961783439490446, 'epoch': 0.19}
  File "C:\Users\Ryan\Desktop\2025 Spring\CS 7643 - Deep Learning\Final project\Code\titan-project\train_mistral_lora.py", line 115, in <module>
    main()
  File "C:\Users\Ryan\Desktop\2025 Spring\CS 7643 - Deep Learning\Final project\Code\titan-project\train_mistral_lora.py", line 108, in main
    trainer.train()
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 2245, in train
    return inner_training_loop(
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 2627, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 3096, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 3045, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 4154, in evaluate
    output = eval_loop(
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 4348, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\trainer.py", line 4574, in prediction_step
    outputs = model(**inputs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\accelerate\utils\operations.py", line 814, in forward
    return model_forward(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\accelerate\utils\operations.py", line 802, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\amp\autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\peft\peft_model.py", line 2577, in forward
    return self.base_model(
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\peft\tuners\tuners_utils.py", line 193, in forward
    return self.model.forward(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\utils\generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\utils\deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\models\mistral\modeling_mistral.py", line 830, in forward
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\loss\loss_utils.py", line 63, in ForCausalLMLoss
    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\transformers\loss\loss_utils.py", line 35, in fixed_cross_entropy
    loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
  File "C:\Users\Ryan\anaconda3\envs\titan_env\lib\site-packages\torch\nn\functional.py", line 3479, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.95 GiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 23.51 GiB is allocated by PyTorch, and 2.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
