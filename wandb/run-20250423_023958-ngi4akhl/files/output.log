Loading checkpoint shards: 100%|█████████████████████████████████████████████████████| 2/2 [01:16<00:00, 38.38s/it]
trainable params: 19,922,944 || all params: 7,261,655,040 || trainable%: 0.2744
Loading 1000 examples from TriviaQA dataset...
Resolving data files: 100%|████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 57.08it/s]
Resolving data files: 100%|████████████████████████████████████████████████████| 26/26 [00:00<00:00, 167772.16it/s]
Loaded 900 training examples and 100 validation examples
Filter: 100%|████████████████████████████████████████████████████████████| 900/900 [00:08<00:00, 109.52 examples/s]
Filter: 100%|███████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1152.59 examples/s]
After filtering: 861 training examples and 97 validation examples
Map: 100%|██████████████████████████████████████████████████████████████| 861/861 [00:00<00:00, 7282.88 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 5331.58 examples/s]
Tokenizing datasets...
Map: 100%|██████████████████████████████████████████████████████████████| 861/861 [00:00<00:00, 1560.94 examples/s]
Map: 100%|████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 1787.28 examples/s]
Final training examples: 861
Final validation examples: 97
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|                                                                                       | 0/81 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 12%|█████████▍                                                                  | 10/81 [08:54<1:02:09, 52.53s/it]Traceback (most recent call last):
{'loss': 1.9927, 'grad_norm': 5.672061443328857, 'learning_rate': 0.0001871794871794872, 'epoch': 0.37}
  File "/storage/ice1/0/0/rstonick3/titan-project/train_mistral_lora.py", line 115, in <module>33<00:11,  3.82s/it]
    main()
  File "/storage/ice1/0/0/rstonick3/titan-project/train_mistral_lora.py", line 108, in main
    trainer.train()
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer.py", line 2627, in _inner_training_loop
    self._maybe_log_save_evaluate(
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer.py", line 3096, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer.py", line 3045, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer.py", line 4154, in evaluate
    output = eval_loop(
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer.py", line 4375, in evaluation_loop
    all_preds.add(logits)
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 316, in add
    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 130, in nested_concat
    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)
  File "/home/hice1/rstonick3/scratch/titan_env_vm/lib/python3.10/site-packages/transformers/trainer_pt_utils.py", line 88, in torch_pad_and_concatenate
    return torch.cat((tensor1, tensor2), dim=0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 21.48 GiB. GPU 0 has a total capacity of 44.43 GiB of which 17.68 GiB is free. Including non-PyTorch memory, this process has 26.74 GiB memory in use. Of the allocated memory 26.06 GiB is allocated by PyTorch, and 179.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
